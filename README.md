# distributed-training-coding-everything

> 纸上得来终觉浅，绝知此事要躬行。--陆游\
> Shallow learning from books will not lead to a deep understanding; real knowledge comes from practical experience.

We use the python, pytorch, pytorch.distributed to implement the famous LLM-distributed-training-methods.

## TODO 

- [ ] Using NCCL to learn pytorch distributed
- [ ] Complete a series simple tool like backpropagation, optimizer
- [ ] Naive Implement of DDP
- [ ] more....

## Introduce

We will try our best to coding everything to implement the distribued model training in deep learning. 

Althouth we use PyTorch, we will try to avoid using encapsulated advanced features as much as possible, such as automatic differentiation, optimizers, gradient updates, etc.

## Module Description

In these modules, except for the public tools module, all other modules are independent of each other.

This is my learning way, from the basic to complexed features.

### Public Tools

Path: `src/public_tools`


### Pytorch Distributed 

Path: `src/pytorch_distributed`

### Perceptrons and Neural Networks

Path: ``

### Optimizer

Path: ``


### Trainer

Path: ``

### DDP

Path: ``

### MP

Path: ``

### PP

Path: ``

### FSDP

Path: ``

### DeepSpeed

Path: ``



